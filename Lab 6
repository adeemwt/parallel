#include "cuda_runtime.h"
#include "device_launch_parameters.h"
#include <stdio.h>
#include <omp.h>
#include <stdlib.h>
#include <time.h>
#include <string.h>
#define nBlocks 1
#define  numOfThreads 64 /**** IMPORTANT: The number of threads should be 2^n otherwise it wont run properlay...  ***/
void printIntArray(const int *array, int size);
void generateRandomIntArray(int *numbers, int size, int max);
cudaError_t createHistParallel(int* numbers, int size, int *hist, int histSize);
void createHistParallel_(const int* numbers, int size, int *hist, int histSize);
void printIntArray(const int *array, int size);
void  TIMER(cudaEvent_t start,cudaEvent_t end);
float TIMERvalue(cudaEvent_t start,cudaEvent_t end);
__global__ void addTotalKernel(int *hist, int *numbers,int size,int * allhists);

__global__ void addKernel(int *hist, int *numbers,int size,int * allhists)
{
	int tid = threadIdx.x,i,hist_size = numOfThreads*size,step=tid*size;
	for(i = step; i < step+size; i++)
	{
		allhists[numbers[i] + tid*hist_size]++;
	}
}
__global__ void addTotalKernel(int *hist, int *numbers,int size,int * allhists)
{	
	int tid = threadIdx.x,hist_size = size*numOfThreads,i;
	for(i = tid*size; i < tid*size+size; i++)
	{
		for(int j = 0; j < numOfThreads; j++)
		{
			hist[i] += allhists[i + j *hist_size];
		}
	}
}
void generateRandomIntArray(int *numbers, int size, int max)
{
	for(int i = 0; i < size; i++)
		numbers[i] = rand() % max;
}
int main()
{
	int* arr = (int*)malloc(256*sizeof(int));
	int arrSize = 256;
	generateRandomIntArray(arr,arrSize,arrSize);
	int* hist = (int*)malloc(sizeof(int)*arrSize);
	int* hist_2 = (int*)malloc(sizeof(int)*arrSize);
	// Add vectors in parallel.
	cudaEvent_t start;
	cudaEvent_t end;
	cudaEventCreate( &start );
	cudaEventCreate( &end );
	TIMER(start,end);
	cudaError_t cudaStatus = createHistParallel(arr, arrSize, hist,arrSize);
	if (cudaStatus != cudaSuccess) {
		fprintf(stderr, "addWithCuda failed!");
		return 1;
	}

	float totalTimeC= TIMERvalue(start,end);
	TIMER(start,end);
	createHistParallel_(arr, arrSize,hist_2,arrSize);
	float totalTimeOMP = TIMERvalue(start,end);
	// cudaDeviceReset must be called before exiting in order for profiling and
	// tracing tools such as Nsight and Visual Profiler to show complete traces.
	cudaStatus = cudaDeviceReset();
	if (cudaStatus != cudaSuccess) {
		fprintf(stderr, "cudaDeviceReset failed!");
		return 1;
	}
	printf("made by Cuda::\n");
	printIntArray(hist, arrSize);
	printf("____________________________________\nmade by omp::\n");
	printIntArray(hist_2, arrSize);
	int Pass =0;
#pragma omp for shared(Pass)
	for(int i =0 ; i < arrSize;i++){
		if(hist_2[i] != hist[i]){
			Pass =1;
			printf("%d , %d ",hist_2[i],hist[i]);
		}
	}
	if(Pass ==0)
		printf("Pass.");
	else
		printf("Fail.");
	printf("\n");
	printf("Total CUDA time :  %f  ms.\nTotal OMP run time :  %f  ms.",totalTimeC,totalTimeOMP);
	getchar();
	return 0;
}
// Helper function for using CUDA to add vectors in parallel.
cudaError_t createHistParallel(int* numbers, int size, int *hist, int histSize)
{

	printf("Running Cuda, allocating memory...");
	int *Cnumbers ,*Chist,jumpSize = 256/ numOfThreads;
	cudaError_t cudaStatus;
	cudaEvent_t start, end;
	cudaEventCreate( &start );
	cudaEventCreate( &end );
	TIMER(start,end);
	int *allHists = (int*) calloc(histSize * numOfThreads, sizeof(int)),*c_allHists;
	// Choose which GPU to run on, change this on a multi-GPU system.
	cudaStatus = cudaSetDevice(0);
	if (cudaStatus != cudaSuccess) {
		fprintf(stderr, "cudaSetDevice failed!  Do you have a CUDA-capable GPU installed?");
	}

	// Allocate GPU buffers for three vectors (two input, one output)    .
	cudaStatus = cudaMalloc((void**)&Cnumbers, size * sizeof(int));

	cudaStatus = cudaMalloc((void**)&Chist, size * sizeof(int));
	
	cudaStatus = cudaMalloc((void**)&c_allHists, size * sizeof(int)*numOfThreads);
	
	cudaStatus = cudaMemcpy(Cnumbers, numbers, size * sizeof(int), cudaMemcpyHostToDevice);

	cudaStatus = cudaMemcpy(c_allHists,allHists, histSize * numOfThreads* sizeof(int), cudaMemcpyHostToDevice);
	
	// Copy input vectors from host memory to GPU buffers.
	// Launch a kernel on the GPU with one thread for each element.

	printf("\nFinished allocating memory, time takes : %f ms.\nRunning kernel...",TIMERvalue(start,end));
	TIMER(start,end);
	addKernel<<<nBlocks, numOfThreads>>>(Chist, Cnumbers,  jumpSize,c_allHists);
	addTotalKernel<<<nBlocks, numOfThreads>>>(Chist, Cnumbers,  jumpSize,c_allHists);
	printf("\nFinished Running kernel, time taken : %f ms.\n",TIMERvalue(start,end));

	// cudaDeviceSynchronize waits for the kernel to finish, and returns
	// any errors encountered during the launch.
	cudaStatus = cudaDeviceSynchronize();
	
	// Copy output vector from GPU buffer to host memory.
	cudaStatus = cudaMemcpy(hist, Chist, size * sizeof(int), cudaMemcpyDeviceToHost);
	cudaFree(Chist);
	cudaFree(Cnumbers);
	return cudaStatus;

}
void createHistParallel_(const int* numbers, int size, int *hist, int histSize)
{	int *allHists, numThreads;
#pragma omp parallel shared(hist, histSize, numbers, size, allHists) 
{
	int tid;
#pragma omp single
	{
		numThreads = omp_get_num_threads();
		allHists = (int*) calloc(histSize * numThreads, sizeof(int));
	}

	tid = omp_get_thread_num();

#pragma omp for
	for(int i = 0; i < size; i++)
	{
		allHists[numbers[i] + tid * histSize]++;
	}

#pragma omp parallel for
	for(int i = 0; i < histSize; i++)
	{
		hist[i] = 0;
		for(int j = 0; j < numThreads; j++)
		{
			hist[i] += allHists[i + j * histSize];
		}
	}

}
free(allHists);
}
void printIntArray(const int *array, int size)
{
	for(int i = 0; i < size; i++)
	{
		if(i % 17 == 0)
		{
			printf("\n");
		}
		printf("%d\t", array[i]);
	}
	printf("\n");
}
void  TIMER(cudaEvent_t start,cudaEvent_t end){
	cudaEventRecord(start, 0); 
}
float TIMERvalue(cudaEvent_t start,cudaEvent_t end){
	float timeValue;
	cudaEventRecord(end, 0);   
	cudaEventSynchronize(end); 
	cudaEventElapsedTime( &timeValue, start, end ); 
	return timeValue;
}
